## Thrashing in Unified Memory 


### CPU-GPU Thrashing

GPU requests a page in managed memory
Page is migrated from CPU to GPU 
CPU request the same page 
Page is migrated back from GPU to CPU 
GPU requests the page again ...

This type of thrashing can occur in iterative applications or
applications with concurrent access to shared data structures.

This can occur with and without GPU oversubscription

#### Access counters and thrashing in Volta 

Access counters keeps track of how many times a page is requested from
CPU. First k requests are serviced as _remote access_ from the CPU
(without migration). Pages are migrated on the k + 1 access. These
pages are labeled as "hot pages". This avoids CPU-GPU thrashing in some
cases. 

It is possible that a page may be hot in both CPU and GPU. Access
counter based migration will not help in that situation. 

#### Duplication 

If access to the offending page is read-only then thrashing can be
avoided by duplication. Copy page A to B. page B is migrated to GPU
while page remained in CPU memory.

#### False Sharing 

If CPU and GPU are accessing different parts of the same thrashing
page then we have a case of false sharing. 

This type of thrashing can be avoided by splitting the accessed data into separate pages. 


### GPU Thrashing

GPU requests page A in managed memory 
Page A is migrated from CPU to GPU 
GPU requests B in managed memory
Page B is migrated from CPU to GPU; A is evicted and migrated back to CPU
GPU requests A again 
Page A is re-migration from CPU to GPU 


This type of thrashing will only occur in GPU over-subscribed applications. 

### Thrashing in Hetero-Mark

_kmeans_ and _fir_ display CPU-GPU thrashing. 

when nvprof reports thrashing numbers for these two applications the
number of CPU thrashes and GPU thrashes are equal, indicating a
ping-pong effect

For _kmeans_, thrashing occurs on the new_membership data structure

For _fir_, thrashing occurs on the history and input data structures


### False Sharing in Unified Memory 

False sharing occurs when CPU and GPU access distinct portion of a shared data structure in
concurrent or nearby phases and the portion of data accessed by the CPU and GPU overlap on a given
page. 

These types of false sharing can occur in GPU oversubscribed iterative applications and in
hybrid applications written in the pipelined and workload partition design paradigm. 
Performance impact can be significant because it is repeated for kernel launch. 


#### Example 1

C\&GI [ template for FIR ]

Assume input buffer size is 2 pages. Then in every iteration there is at least 1 page (4K) extra traffic that
could be avoided if there were no false sharing. The other problem is migration happens in
batches because of prefetching. Thus we can potentially transfer a lot more data with prefetching. 

#### Example 2

WP: [ template for BlackScholes ]


#### Example 3

Pipelined: [ template for EP ]


### Detecting False Sharing 

1. Create a map of the managed space 
     - this is done at runtime
 
2. Calculate the span of each shared data structure within the space
     - the span is measured in number of pages
	 - non-shared data structures can also be allocated in the managed space, although it
       wouldn't make sense to do so. But CUDA allows it. 

3. Label each virtual page as CPU, GPU or shared 


4. All shared pages are candidates for false sharing!

### Create a map of the managed space

1. Count the number of pages allocated in managed space. 

2. For each page find the range of addresses touched by CPU and GPU 
     * range: (begin\_offset, end\_offset)
	 * does not account for irregular/sparse access 
     
3. For each, find the range of addresses that belong to a specific data structure 

4. Identify pages with non-overlapping regions for CPU and GPU access

5. Maps pages back to a data structure 


- driver may merge nearby smaller pages into larger pages on the GPU to improve TLB coverage


  - `--print-gpu-trace` gives us number of faults each virtual page
  - If two warps simultaneously request the same page that is not in GPU memory, both are counted as
    faults
	- how does this happen?
	- requests are supposed to be coalesced before going to L1 TLB?


* TLB miss counters?



  * True sharing can also be problematic. Say, a buffer is being passed back-and-forth between the
    CPU and GPU (e.g., history buffer in FIR). If the buffer is not aligned at page boundaries then
    potentially two extra pages can be migrated back and forth between in every iteration 
	
  * False sharing in FIR in the input buffer. number of pages accessed by host is a subset of the
    pages accessed by the device in the previous iteration. If the portion of the input buffer is
    not aligned at page boundaries, additional data will be migrated back to host in every
    iteration. Draw a picture. 
	
  * False sharing categories
      * non-overlapping range in a page that belongs to the same data structure 
	  * non-overlapping range in a page that belongs to two different data structures 


padding factor: meta data + rounding = ~0.5K



